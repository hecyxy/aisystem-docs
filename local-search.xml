<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>MoCo V3：视觉自监督迎来Transformer</title>
    <link href="/2022/07/20/mocov3/"/>
    <url>/2022/07/20/mocov3/</url>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2104.02057">An Empirical Study of Training Self-Supervised Visual Transformers</a></p><p>何凯明从 CVPR 2020 上发表的 MoCo V1（Momentum Contrast for Unsupervised Visual Representation Learning），到前几天挂在arxiv上面的 MoCo V3（An Empirical Study of Training Self-Supervised Visual Transformers），MoCo一共走过了三个版本。</p><p>今天介绍 MoCo 系列第三版，MoCo v1 和 v2 是针对 CNN 设计的，而 MoCo v3 是针对 Transformer 结构设计的，反映了 MoCo 系列对视觉模型的普适性。</p><h1 id="MoCo-V3-原理分析"><a href="#MoCo-V3-原理分析" class="headerlink" title="MoCo V3 原理分析"></a>MoCo V3 原理分析</h1><p>MoCo v3 的算法原理不应该是这篇论文的重点，这篇论文的重点应该是将目前无监督学习最常用的对比学习应用在 ViT 上。MoCo v3 相比 v2 去掉了 memory queue，转而像SimCLR 那样采用large batch来取得稍好一点的结果，从结构上 encoder f_q 借鉴 BYOL（Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning） 那样增加了一个 prediction head（两层FC），在ResNet上效果有稍许提升。最后通过大量的实验，去证明如何去克服自监督中引入 ViT 训练不稳定的问题。</p><p><img src="/images/mocov3/BYOL.png"></p><h2 id="MoCo-V3-算法原理"><a href="#MoCo-V3-算法原理" class="headerlink" title="MoCo V3 算法原理"></a>MoCo V3 算法原理</h2><p>作者给出的结论是：影响自监督 ViT 模型训练的关键是：instability，即训练的不稳定性。这种训练的不稳定性所造成的结果并不是训练过程无法收敛 (convergence)，而是性能的轻微下降 (下降1%-3%的精度)。</p><p>MoCo v3 损失函数和 v1 和 v2 相同都是使用 InfoNCE，表达式如下：</p><p>$$<br>\mathcal{L}<em>{q}&#x3D;-\log \frac{\exp \left(q \cdot k</em>{+} &#x2F; \tau\right)}{\sum_{i&#x3D;0}^{K} \exp \left(q \cdot k_{i} &#x2F; \tau\right)}<br>\tag{1}<br>$$</p><p>不一样的是 MoCo V3 在网络结构的组成 Framework 有所差异，具体如图所示。因为引入了 ViT 视觉Transformer结构，所以对数据的输入不再是一张完成的图片，而是 image Patch。另外 Transformer对于长序列具有 Attention 的 Q、K、V 结构能够存储和记忆大量的信息，因此取消了 Memory Queue，直接利用大 Batch Size 来进行学习训练。</p><p><img src="/images/mocov3/moco1.png"></p><p>MoCo v3 的训练方法和 MoCo v1&#x2F;2 的训练方法的差异是：</p><ol><li><strong>取消 Memory Queue，用大 Batch Size</strong>：MoCo V3 的 Framework 里面没有 Memory Queue，这就意味着 MoCo v3 所观察的负样本都来自一个 Batch 的图片。换句话讲，只有当 Batch size 足够大时，模型才能看到足够的负样本，所以 MoCo v3 中 Batch size &#x3D; 4096 这样一个巨大的 Batch size。</li></ol><p><img src="/images/mocov3/experience1.png"></p><ol start="2"><li><p><strong>学习 BYOL 添加 Prediction head</strong>：Encoder f_q 除了 Backbone 和预测头 Projection head 以外，还添加了个 Prediction head，是遵循了 BYOL 这篇论文的方法。即 Encoder f_q（ViT(BP) + Projection head + Prediction head），Encoder f_k（ViT(Momentum) + Projection head）。</p></li><li><p><strong>Contrastive loss的更新方式修改</strong>：对于同一张图片的2个增强版本 x1​,x2​ ，分别通过 Encoder f_q​ 和 Momentum Encoder f_k​ 得到 q1​,q2​ 和 k1​,k2​。让 q1​,k2​ 通过 Contrastive loss (式 1) 进行优化 Encoder f_q​的参数，让 q2​,k1​ 通过 Contrastive loss (式 1) 进行优化 Encoder f_q​的参数。Momentum Encoder f_k ​则跟 MoCo V1 版本相同通过动量更新。即：</p></li></ol><p>$$<br>loss &#x3D; Contrastive(q_1, k_2) + Contrastive(q_2, k_1)<br>\tag{2}<br>$$</p><h2 id="MoCo-V3-算法分析"><a href="#MoCo-V3-算法分析" class="headerlink" title="MoCo V3 算法分析"></a>MoCo V3 算法分析</h2><p><img src="/images/mocov3/moco2.png"></p><ol><li>数据增强：</li></ol><p>在ImageNet中 有一堆无标签的数据，拿出一个 MiniBatch，代码表示为 x，也就是 N 张图片，分别进行两种不同的数据增强，得到 x_1 和 x_2，此时 x_1 是 N 张图片，x_2 也是 N 张图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> loader: <span class="hljs-comment"># load a minibatch x with N samples</span><br>    x1, x2 = aug(x), aug(x) <span class="hljs-comment"># augmentation</span><br></code></pre></td></tr></table></figure><ol start="2"><li>分别通过 Encoder 和 Momentum Encoder：</li></ol><p>x_1 分别通过 Encoder 和 Momentum Encoder 得到特征 q_1 和 k_1，维度是 [N,C]，这里特征空间由一个长度为 C&#x3D;128 的向量表示。</p><p>x_2 分别通过 Encoder 和 Momentum Encoder 得到特征 q_2 和 k_2，维度是 [N,C] ，这里特征空间由一个长度为 C&#x3D;128 的向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">q1, q2 = f_q(x1), f_q(x2) <span class="hljs-comment"># queries: [N, C] each</span><br>k1, k2 = f_k(x1), f_k(x2) <span class="hljs-comment"># keys: [N, C] each</span><br></code></pre></td></tr></table></figure><ol start="3"><li>Contrastive loss 的定义：</li></ol><p>对两个维度是 [N,C] 的矩阵（比如是q_1和k_2）做矩阵相乘，得到维度是 [N,N] 的矩阵，其对角线元素代表的就是 positive sample 的相似度，就是让对角线元素越大越好，所以目标是整个 [N,N] 的矩阵越接近单位阵越好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ctr</span>(<span class="hljs-params">q, k</span>):<br>    logits = mm(q, k.t()) <span class="hljs-comment"># [N, N] pairs</span><br>    labels = <span class="hljs-built_in">range</span>(N) <span class="hljs-comment"># positives are in diagonal</span><br>    loss = CrossEntropyLoss(logits/tau, labels)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * tau * loss<br></code></pre></td></tr></table></figure><ol start="4"><li>Contrastive loss 优化：</li></ol><p>通过一个 Contrastive loss 优化 q_1 和 k_2，通过另一个 Contrastive loss 优化 q_2 和 k_1，并反向传播更新 f_q 的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python">loss = ctr(q1, k2) + ctr(q2, k1) <span class="hljs-comment"># symmetrized</span><br>loss.backward()<br><br>update(f_q) <span class="hljs-comment"># optimizer update: f_q</span><br></code></pre></td></tr></table></figure><ol start="5"><li>Momentum Encoder的参数使用动量更新：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">f_k = m*f_k + (<span class="hljs-number">1</span>-m)*f_q <span class="hljs-comment"># momentum update: f_k</span><br></code></pre></td></tr></table></figure><h1 id="MoCo-V3-提升ViT训练稳定性"><a href="#MoCo-V3-提升ViT训练稳定性" class="headerlink" title="MoCo V3 提升ViT训练稳定性"></a>MoCo V3 提升ViT训练稳定性</h1><p>重头戏主要在MoCo v3在ViT上的实验，下面主要是对实验部分进行介绍。由于作者给出的结论是：影响自监督ViT模型训练的关键是：instability，即训练的不稳定性。 而这种训练的不稳定性所造成的结果并不是训练过程无法收敛 (convergence)，而是性能的轻微下降 (下降1%-3%的精度)。下面具体来看看每个实验部分的内容。</p><h2 id="不稳定性测试"><a href="#不稳定性测试" class="headerlink" title="不稳定性测试"></a>不稳定性测试</h2><ol><li>Batch size 过大使得训练不稳定</li></ol><p>如下图，Encoder 架构换成 ViT-B&#x2F;16 ，Learning rate&#x3D;1e-4，在 ImageNet 数据集上训练 100 epochs 的结果。作者使用了4种不同的 Batch size：1024, 2048, 4096, 6144 的结果。可以看到当 bs&#x3D;4096 时，曲线出现了 dip 现象 (稍稍落下又急速升起)。这种不稳定现象导致了精度出现下降。当 bs&#x3D;6144 时，曲线的 dip 现象更大了，可能是因为跳出了当前的 local minimum。这种不稳定现象导致了精度出现了更多的下降。</p><p><img src="/images/mocov3/experience1.png"></p><p>正常在CNN架构上，Batch size越大，那么自监督学习中负样本数量越大，能够学习更多的负样本特征，但是使用 ViT结构取消 Memory Queue，用大 Batch Size后却出现了 dip 现象，继续实验。</p><ol start="2"><li>Learning rate 过大使得训练不稳定</li></ol><p>如下图，Encoder 架构换成 ViT-B&#x2F;16 ，Batch size&#x3D;4096，在 ImageNet 数据集上训练 100 epochs 的结果。作者使用了4种不同的 Learning rate：0.5e-4, 1.0e-4, 1.5e-4 的结果。可以看到当Learning rate 较大时，曲线出现了 dip 现象 (稍稍落下又急速升起)。这种不稳定现象导致了精度出现下降。</p><ol start="3"><li>LARS optimizer 的不稳定性</li></ol><p>如下图，使用了 LARS 优化器，分别使用了4种不同的 Learning rate：3e-4, 5e-4, 6e-4, 8e-4 的结果。结果发现当给定合适的学习率时，LARS的性能可以超过AdamW，但是当学习率稍微变大时，性能就会显著下降。而且曲线自始至终都是平滑的，没有 dip 现象。所以最终为了使得训练对学习率更鲁棒，作者还是采用 AdamW 作为优化器。因为若采用 LARS，则每换一个网络架构就要重新搜索最合适的 Learning rate。</p><p><img src="/images/mocov3/experience2.png"></p><h2 id="提升稳定性的方法"><a href="#提升稳定性的方法" class="headerlink" title="提升稳定性的方法"></a>提升稳定性的方法</h2><p>既然上面重点分析了 Batch Size、Learning rate 和 LARS optimizer对不稳定性的分析，论文实验中就给出了提升稳定性的方法：<strong>random patch projection</strong>。</p><p>导致训练出现不稳定的这些 dip 跟梯度暴涨 (spike) 有关，第1层会先出现梯度暴涨的现象，结果几十次迭代后，会传到到最后1层。作者解决的办法是冻结第1层的参数，也就是patch embedding那层，随机初始化后，不再更新这一层的参数。</p><p>实验结果通过下面的图可以看到，使用 MoCo v3 or SimCLR, BYOL 方法，Encoder 架构换成 ViT-B&#x2F;16 ，Batch size&#x3D;4096，在 ImageNet 数据集上训练 100 epochs 的结果，不同的是冻结了patch embedding那层的参数，使用了随机参数初始化。</p><p>不论是 MoCo v3 还是 SimCLR, BYOL 方法，冻结 patch embedding 那层的参数都能够提升自监督 ViT 的训练稳定性。除此之外， gradient-clip 也能够帮助提升训练稳定性，其极限情况就是冻结参数，真的可以避免出现 dip 现象。</p><p><img src="/images/mocov3/experience3.png"></p><h1 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h1><p>有意思的是这篇论文的写作方式，首先通过实验发现问题，然后通过实验去解决问题，然后再做一些对比实验，深度学习发文章真的是实验对比非常重要。</p><p>作者对 1) position embedding 的具体形式，2) class token 的必要性，3) Prediction head 的必要性和 4) momentum 超参数的影响分别做了不同的对照实验。其中 position embedding 和 class token 的影响分析比较重要，另外两个对精度影响在千分之一之间。</p><ol><li>位置编码的具体形式</li></ol><p>在 Transformer 结构里面position embedding很重要，在无监督训练过程去除位置编码，效果下降了1个多点，说明 ViT 的学习能力很强，在没有位置信息的情况下就可以学习的很好；从另外一个角度来看，也说明 ViT 并没有充分利用好位置信息。</p><p><img src="/images/mocov3/experience4.png"></p><ol start="2"><li>class token 的必要性</li></ol><p>使用 class token 的性能是76.5，而简单地取消 class token，并换成 Global Average Pooling 会下降到69.7，这时候最后一层后面有个LN层。如果把它也去掉，性能会提升到76.3。说明 class token 并不是必要的，LN的选择也很重要。个人猜测 负样本中的 Layer Norm 数据归一化很重要，否则会引起负样本数据不均衡。</p><p><img src="/images/mocov3/experience5.png"></p><p>最后作者跟采用了 Big ResNet 的方法进行对比，以 VIT-L 为backbone的 MoCo v3 完胜。</p><p><img src="/images/mocov3/experience6.png"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>MoCo v3 的改进主要1）取消了 Memory Queue 的机制，2）添加了 Prediction head，3）更新Contrastive loss 优化 Encoder 参数方式。在 自监督学习引入 ViT 的过程中发现了训练不稳定性的问题，通过 random patch embedding 的方式暂时解决了目前问题，但是更大的 Batch Size仍然会引起 dip 问题还需要进一步研究啦。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] Hadsell, Raia, Sumit Chopra, and Yann LeCun. “Dimensionality reduction by learning an invariant mapping.” 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). Vol. 2. IEEE, 2006.</p><p>[2] Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” International conference on machine learning. PMLR, 2020.</p><p>[3] He, Kaiming, et al. “Momentum contrast for unsupervised visual representation learning.” Proceedings of the IEEE&#x2F;CVF conference on computer vision and pattern recognition. 2020.</p><p>[4] Chen, Xinlei, et al. “Improved baselines with momentum contrastive learning.” arXiv preprint arXiv:2003.04297 (2020).</p><p>[5] Grill, Jean-Bastien, et al. “Bootstrap your own latent-a new approach to self-supervised learning.” Advances in neural information processing systems 33 (2020): 21271-21284.</p><p>[6] <a href="https://zhuanlan.zhihu.com/p/364446773">https://zhuanlan.zhihu.com/p/364446773</a></p><p>[7] <a href="https://www.zhihu.com/question/453203448/answer/1843542119">https://www.zhihu.com/question/453203448/answer/1843542119</a></p><p>[8] <a href="https://zhuanlan.zhihu.com/p/382763210">https://zhuanlan.zhihu.com/p/382763210</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MoCo V2：MoCo系列再升级</title>
    <link href="/2022/07/20/mocov2/"/>
    <url>/2022/07/20/mocov2/</url>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2003.04297">Improved Baselines with Momentum Contrastive Learning</a></p><p>何凯明从 CVPR 2020 上发表的 MoCo V1（Momentum Contrast for Unsupervised Visual Representation Learning），到前几天挂在arxiv上面的 MoCo V3（An Empirical Study of Training Self-Supervised Visual Transformers），MoCo一共走过了三个版本。</p><p>今天介绍 MoCo 系列第二版 MoCo v2 就是在 SimCLR 发表后结合了 SimCLR 优点的图像自监督学习方法，MoCo v1 和 v2 是针对 CNN 设计的，而 MoCo v3 是针对 Transformer 结构设计的，反映了 MoCo 系列对视觉模型的普适性。</p><h1 id="MoCo-V2-的改进"><a href="#MoCo-V2-的改进" class="headerlink" title="MoCo V2 的改进"></a>MoCo V2 的改进</h1><p>在 SimCLR v1 发布以后，MoCo的作者团队就迅速地将 SimCLR 的两个提点的方法移植到了 MoCo 上面，想看下性能的变化，也就是 MoCo v2。结果显示，MoCo v2 的结果取得了进一步的提升并超过了 SimCLR v1，证明 MoCo 系列方法的地位。因为 MoCo v2 文章只是移植了 SimCLR v1 的技巧而没有大的创新，所以作者就写成了一个只有2页的技术报告，还有长长的一页引用了大量文章。</p><p>有兴趣的读者可以参考MoCo V2的文章，Improved Baselines with Momentum Contrastive Learning。</p><h2 id="MoCo-V2-相关工作"><a href="#MoCo-V2-相关工作" class="headerlink" title="MoCo V2 相关工作"></a>MoCo V2 相关工作</h2><p>动量对比（MoCo V1）表明，无监督预训练可以在多个检测和分割任务中超过其图像监督的预训练，而 SimCLR 进一步减少了无监督和监督预训练表示之间的线性分类器性能差距。</p><p><img src="/images/mocov2/moco1.png"></p><p>SimSLR仍然采用端到端的方法，如图（a）的方式，不过在三个方面改进了实例识别的端到端变体：(i)一个更大的批(4k或8k)，可以提供更多的负样本；(ii)用MLP头替换输出fc投影头；(iii)更强的数据增强。</p><p>在 SimCLR 中具体的来说，就是使用强大的数据增强策略，额外使用了 Gaussian Deblur 的策略和使用巨大的 Batch size，让自监督学习模型在训练时的每一步见到足够多的负样本 (negative samples)，这样有助于自监督学习模型学到更好的 visual representations。</p><p>使用预测头 Projection head。在 SimCLR 中，Encoder 得到的2个 visual representation再通过Prediction head 进一步提特征，预测头是一个 2 层的 MLP，将 visual representation 这个 2048 维的向量 h_i, h_j 进一步映射到 128 维隐空间中，得到新的representation z_i, z_j。利用表征向量 z_i, z_j 去求 Contrastive loss 完成训练，训练完毕后扔掉预测头，保留 Encoder 用于获取 visual representation。</p><p><img src="/images/mocov2/moco2.png"></p><p>我们继续根据 end-to-end 的方法来继续展开。图中 End-to-end 的方法：一个Batch的数据假设有 N 张 image，这里面有一个样本 query q 和它所对应的正样本 k+， q 和 k+ 来自同一张图片的不同的 Data Augmentation，这个Batch剩下的数据就是负样本 (negative samples)。接着将这个 Batch 的数据同时输入给2个架构相同但参数不同的 Encoder f_q 和 Encoder f_k。然后对两个 Encoder的输出使用 Contrastive loss 损失函数使得 query q 和正样本 k+ 的相似程度尽量地高，使得 query q 和负样本 k- 的相似程度尽量地低，通过这样来训练Encoder f_q 和 Encoder f_k，这个过程就称之为自监督预训练。训练完毕后得到的 Encoder 的输出就是图片的 visual representation。</p><p>End-to-end 方法的缺点是：因为 Encoder f_q 和 Encoder f_k 的参数都是通过反向传播来更新的，所以 Batch size 的大小不能太大，否则 NPU 显存就不够了。Batch size 的大小限制了负样本的数量，也限制了自监督模型的性能。SimCLR 是 Google 提出的，有庞大的TPU集群加持，肯定不愁吃不愁穿，但是普通老百姓肯定不能这样。</p><h2 id="MoCo-V2-直接上实验"><a href="#MoCo-V2-直接上实验" class="headerlink" title="MoCo V2 直接上实验"></a>MoCo V2 直接上实验</h2><p>回到今天的主角身边，MoCo v2 的亮点是不需要强大的 Google TPU 加持，仅仅使用 8-GPU 就能超越 SimCLR v1 的性能。v2 将 SimCLR的两个提点的方法 (a 使用预测头 b 使用强大的数据增强策略) 移植到了 MoCo v1上面，实验如下。</p><p><strong>训练集</strong>：ImageNet 数据集。</p><p><strong>评价手段</strong>：</p><ol><li><p>Linear Evaluation：Encoder (ResNet-50) 的参数固定不动，在Encoder后面加分类器（具体就是一个FC层+softmax激活函数），使用全部的 ImageNet label 只训练分类器的参数，而不训练 Encoder 的参数)。看最后 Encoder+分类器的性能。</p></li><li><p>VOC 目标检测 使用 Faster R-CNN 检测器 (C4 backbone)，在 VOC 07+12 trainval set 数据集进行 End-to-end 的 Fine-tune。在 VOC 07 test 数据集进行 Evaluation。</p></li></ol><h3 id="使用预测头"><a href="#使用预测头" class="headerlink" title="使用预测头"></a><strong>使用预测头</strong></h3><p>预测器 Projection head 分类任务的性能只存在于自监督的预训练过程，在 Linear Evaluation 和下游任务中都是被去掉的。MoCo V1 的 Encoder 简单使用了 ResNet50，然后输出通过 L2-norm 处理得到最后的输出。在 MoCo V2 中把 ResNet 中输出与1000分类相关的FC层换成了两层的 FC + Relu，隐藏层为2048维。</p><p>Linear Evaluation 结果如下图：</p><p><img src="/images/mocov2/experience1.png"></p><p>图中的 τ 就是损失函数对应的 τ 。在使用预测器且 τ&#x3D;0.07 时精度从 from 60.6% to 62.9%。</p><h3 id="数据增强策略"><a href="#数据增强策略" class="headerlink" title="数据增强策略"></a><strong>数据增强策略</strong></h3><p>对数据增强策略，作者在 MoCo v1 的基础上又添加了 blur augmentation，发现更强的色彩干扰作用有限。只添加 blur augmentation 就可以使得 ImageNet 分类任务的性能从 60.6% 增长到 63.4%，再加上预测头 Projection head 就可以使性能进一步涨到67.3%。</p><p><img src="/images/mocov2/experience2.png"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>MoCo v2 把 SimCLR 中的两个主要提升方法：1）使用强大的数据增强策略，具体就是额外使用了 Gaussian Deblur 的策略；2）使用预测头 Projection head 到 MoCo 中，并且验证了 SimCLR 算法的有效性。最后 MoCo v2 的结果更优于 SimCLR v1，证明 MoCo 系列自监督预训练方法的高效性。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] Hadsell, Raia, Sumit Chopra, and Yann LeCun. “Dimensionality reduction by learning an invariant mapping.” 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). Vol. 2. IEEE, 2006.</p><p>[2] Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” International conference on machine learning. PMLR, 2020.</p><p>[3] He, Kaiming, et al. “Momentum contrast for unsupervised visual representation learning.” Proceedings of the IEEE&#x2F;CVF conference on computer vision and pattern recognition. 2020.</p><p>[4] Chen, Xinlei, et al. “Improved baselines with momentum contrastive learning.” arXiv preprint arXiv:2003.04297 (2020).</p><p>[5] <a href="https://zhuanlan.zhihu.com/p/364446773">https://zhuanlan.zhihu.com/p/364446773</a></p><p>[6] <a href="https://zhuanlan.zhihu.com/p/469100381">https://zhuanlan.zhihu.com/p/469100381</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MoCo V1：视觉领域也能自监督啦</title>
    <link href="/2022/07/20/mocov1/"/>
    <url>/2022/07/20/mocov1/</url>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1911.05722">Momentum Contrast for Unsupervised Visual Representation Learning</a></p><p>何凯明从 CVPR 2020 上发表的 MoCo V1（Momentum Contrast for Unsupervised Visual Representation Learning），到前几天挂在arxiv上面的 MoCo V3（An Empirical Study of Training Self-Supervised Visual Transformers），MoCo一共走过了三个版本。</p><p>今天介绍 MoCo 系列第一版 MoCo v1 就是在 SimCLR 发表前经典的图像自监督学习方法，MoCo v1 和 v2 是针对 CNN 设计的，而 MoCo v3 是针对 Transformer 结构设计的，反映了 MoCo 系列对视觉模型的普适性。</p><h1 id="自监督学习-Self-Supervised-Learning"><a href="#自监督学习-Self-Supervised-Learning" class="headerlink" title="自监督学习 Self-Supervised Learning"></a>自监督学习 Self-Supervised Learning</h1><p>一般机器学习分为有无监督学习，无监督学习和强化学习。而自监督学习（Self-Supervised Learning）是无监督学习里面的一种，主要是希望能够学习到一种通用的特征表达用于下游任务 (Downstream Tasks)。而在视觉模型中，MoCo 之所以经典是创造出了一个固定的视觉自监督的模式：</p><blockquote><p><strong>Unsupervised Pre-train, Supervised Fine-tune.</strong></p><p><strong>预训练模型使用自监督方法，下游任务使用监督方法微调</strong></p></blockquote><p><img src="/images/mocov1/self_supervised1.png"></p><p>对应图中，预训练阶段使用无标签的数据集 (unlabeled data)，因为带标签的（labeled data）数据收集非常昂贵，需要大量的新一代农民工去标注，成本是相当高。<br>相反，无标签的数据集收集很方便，不需要大量的新一代农民工。</p><p><img src="/images/mocov1/farmer.jpg"></p><p>在无监督CV领域，第一阶段叫做in a task-agnostic way，在训练模型参数的时候，Self-Supervised Learning 就想不用带标签的数据，先把初始化网络模型的权重参数训练到基本可用，得到一个中间权重参数结果，我们把它叫做 Visual Representation。</p><p>第二阶段叫做in a task-specific way，根据下游任务 (Downstream Tasks) 使用带标签的数据集把参数训练到精度达标，这时使用的数据集量就不用太多了，因为参数经过了阶段一的预训练啦。</p><p>MoCo 遵循这个思想，预训练的 MoCo 模型也会得到 Visual Representation，然后通过 Fine-tune 以适应各种各样的下游任务（比如目标检测、语义分割等）。下面图中的实验结果表明，MoCo在 7 个检测&#x2F;语义分割任务（PASCAL VOC, COCO, 其他的数据集）上可以超过了监督学习训练版本。</p><p><img src="/images/mocov1/experience1.png"></p><p>自监督学习的关键可以概括为两点：Pretext Task，Loss Function，在下面分别介绍。</p><h2 id="Contrastive-loss"><a href="#Contrastive-loss" class="headerlink" title="Contrastive loss"></a>Contrastive loss</h2><p>Contrastive loss 来自于 2006年 Yann LeCun 组的工作（Dimension- ality reduction by learning an invariant mapping）。</p><p>Contrastive loss 的思想是想让：1）相近的样本之间的距离越小越好。2）不似样本之间的距离如果小于m，则通过互斥使其距离接近m。文章对第二个点有个形象的解释，就像长度为m的弹簧，如果它被压缩，则会因为斥力恢复到长度m。</p><p><img src="/images/mocov1/loss1.png"></p><p>$$<br>L(W,Y,\vec{X_1},\vec{X_2})&#x3D;(1-Y)\frac{1}{2}(D_w)^2+(Y) \frac{1}{2} { max(0, m-D_w) }^2<br>\tag{1}<br>$$</p><p>其中 W 是网络权重；Y 是成对标签，如果 X1，X2 这对样本属于同一个类，Y&#x3D;0，属于不同类则 Y&#x3D;1。Dw 是 X1 与 X2 在潜变量空间的欧几里德距离。当 Y&#x3D;0，调整参数最小化X1与X2 之间的距离。当 Y&#x3D;1，如果 X1与X2 之间距离大于 m，则不做优化；如果 X1 与 X2 之间的距离小于 m, 则增大两者距离到 m。</p><p>最后的实际效果就像论文给出的实验结果，训练完后在Mnist手写字体数据集上4和9明确的分开出来了。</p><p><img src="/images/mocov1/loss2.png"></p><h2 id="Pretext-Task"><a href="#Pretext-Task" class="headerlink" title="Pretext Task"></a>Pretext Task</h2><p>Pretext Task（译作：借口、托辞）是无监督学习领域的一个常见的术语，专指通过完成暂时的任务A，能够对后续的任务B、C、D有帮助。下面针对NLP和CV有两种主要的Pretext模式。</p><ol><li>NLP领域的 Pretext Task：在训练 BERT 的时候，预训练过程进行作填空的任务。</li></ol><p>如下图所示，把输入文字里面的一部分随机盖住，就是直接用一个掩码 Mask 把要盖住的token（字符或者一个字）给遮盖住，换成一个特殊的字符。接下来把这个盖住的 token 对应位置输出的向量执行线性变换 Linear Transformation，对输出执行softmax计算输出关于每一个字的概率分布。因为这时候 BERT 并不知道被掩盖住的字是 “湾” ，但是输入的原始数据是知道这个信息的，所以损失就是让这个输出和被盖住的 “湾” 越接近越好。这个任务和下游任务毫不相干，但是 BERT 就是通过 Pretext Task 学习到了很好的 Language Representation 作为预训练模型，很好地适应了下游任务。</p><p><img src="/images/mocov1/self_supervised2.png"></p><p>(2) CV领域的 Pretext Task：在训练 SimCLR 的时候，预训练过程让模型区分相似和不相似的图像。</p><p>如下图所示，假设现在有1张图片 x ，先对 x 进行数据增强，得到2张增强以后的图片 x_i, x_j 。接下来把增强后的图片 x_i, x_j 输入到Encoder里面，注意这2个Encoder是共享参数的，得到representation h_i 和 h_j ，再把 h_i 和 h_j 通过 Projection head 得到 representation z_i 和 z_j。下面的目标就是最大化同一张图片得到的 z_i 和 z_j ，最小化不同张图片得到的 z_i 和 z_j。其具体的结构表达式是：</p><p>$$<br>z_i &#x3D; g(h+i) &#x3D; W_2 \sigma(W_1 h_i) \tag{2}<br>$$</p><p><img src="/images/mocov1/self_supervised3.png"></p><p>通过上图方式训练视觉模型，学习到了很好的视觉预训练模型的表达 Image Representation，在下游任务只要稍微进行 Fine-tune，效果就会比有很大的提升。</p><h1 id="MoCo-V1对自监督的改进"><a href="#MoCo-V1对自监督的改进" class="headerlink" title="MoCo V1对自监督的改进"></a>MoCo V1对自监督的改进</h1><p>整篇文章其实主要是在介绍如何用对比学习去无监督地学习视觉的表征。</p><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>先考虑一个任务，现在有两个图片，图片1和图片2。先在图片1中通过数据增益产生两张图片，记作A，B，在图片2中截出一个patch记作C，现在把B和C放到样本库里面，样本库图片的位置随机打乱，然后以A作为查找的对象，让你从样本库中找到与A对应的图片。</p><p><img src="/images/mocov1/moco4.png"></p><p>假设随机裁剪了A，B， C三个图，然后将A设为被预测的对象，然后A通过encoder1编码为向量q，接着B、C经过encoder2编码为k1和k2。q和k1算相似度得到S1，q和k2算相似度得到S2。我们的目的是想要让机器学出来A和B是一类(关联性强)，而A和C其它不是(关联性弱)。</p><p><img src="/images/mocov1/moco5.png"></p><p>由于提前知道A和B是同一张图截出来的，而C不是，因此希望S1（A和B的相似度）尽可能高而S2（A和C的相似度）尽可能低。把B打上是正类的标签，把C打上是负类的标签，即同一张图片截出来的patch彼此为正类，不同的图片截出来的记为负类，由于这种方式只需要设定一个规则，然后让机器自动去打上标签，基于这些自动打上的标签去学习，所以也叫做自监督学习，MoCo就是通过不需要借助手工标注去学习视觉表征。</p><p>MoCo通过构建一个动态的负类队列来进行对比学习，依旧通过上面的例子来说，一般要学到好的表征需要比较多的负类样本，但是由于计算资源限制又不能加入太多的负类样本，并且我们也不希望负类样本是一成不变的，因此提出了就有了 dynamic dictionary with a queue。</p><p><img src="/images/mocov1/moco1.png"></p><p>x^query可以类比上面的图A，x^key类比是图B和图C，图中的encoder可以是CNN，queue就是样本队列，剩下momentum encoder和contrastive loss。</p><h2 id="contrastive-loss"><a href="#contrastive-loss" class="headerlink" title="contrastive loss"></a>contrastive loss</h2><p>对比学习关注的是能不能区别出同类和非同类的样本，Contrastive loss有很多不同的形式，MoCo使用的是InfoNCE，表达式如下：</p><p>$$<br>\mathcal{L}<em>{q}&#x3D;-\log \frac{\exp \left(q \cdot k</em>{+} &#x2F; \tau\right)}{\sum_{i&#x3D;0}^{K} \exp \left(q \cdot k_{i} &#x2F; \tau\right)}<br>\tag{3}<br>$$</p><p>这里通过点积来计算 q 和 k 的相似度，k+ 是指正样本经过momentum encoder编码成的向量，注意的是里面对照样本里面只有一个正样本，其余都是负样本，至于分母 τ 就是softmax的温度参数，用来控制概率分布的尖锐和平滑。</p><h2 id="momentum-encoder"><a href="#momentum-encoder" class="headerlink" title="momentum encoder"></a>momentum encoder</h2><p>原始的自监督学习方法里面的这一批负样本就相当于是有个字典 （Dictionary），字典的key就是负样本，字典的value就是负样本通过 Encoder 之后得到的特征向量。</p><p>那么现在问题来了：这一批负样本，即字典的大小是多大呢？</p><p>负样本的规模就是 batch size，即字典的大小就是 batch size。</p><p>举个例子，假设 batch size &#x3D; 256，那么对于给定的一个样本 ，选择一个正样本 （经过data augmentation的图像）。然后选择256个负样本，然后使用 loss function 来将与正样本之间的距离拉近，负样本之间的距离推开到系数m。</p><p>毫无疑问是 batch size 越大效果越好的，这一点在 SimCLR 中也得到了证明。但是，由于硬件的影响 batch size 不能设置过大，因此很难应用大量的负样本。因此效率较低，如图（a）。</p><p>于是图（b）采用一个较大的memory bank存储较大的字典：对于给定的一个样本 ，选择一个正样本 （经过data augmentation的图像）。采用一个较大的 memory bank 存储较大的字典，这个 memory bank 具体存储的是所有样本的表征 representation（涵盖所有的样本，比如样本一共有60000个，那么memory bank大小就是60000，字典大小也是60000）。采样其中的一部分负样本 ，然后使用Contrastive loss将 q 与正样本之间的距离拉近，负样本之间的距离推开。这次只更新 Encoder 的参数，和采样的key值 。因为这时候没有了 Encoder 的反向传播，所以支持memory bank容量很大。</p><p>但是，这一个step更新的是 Encoder 的参数，和几个采样的key值 ，下个step更新的是 Encoder 的参数，和几个采样的key值 ，Encoder 的参数每个step都更新，但是某一个 key 可能很多个step才被采样到更新一次，而且一个epoch只会更新一次。这就出现了一个问题：每个step编码器都会进行更新，这样最新的 query 采样得到的 key 可能是好多个step之前的编码器编码得到的 key，因此丧失了一致性。</p><p>从这一点来看，（a）端到端自监督学习方法的一致性最好，但是受限于batchsize的影响。而（b）采用一个memory bank存储较大的字典，一致性却较差。</p><p><img src="/images/mocov1/moco2.png"></p><p>实现对比学习可以有以上三种形式。在(a)中，encoder q和encoder k都是端对端一起训练，encoder q和encoder k可以是两个不同的网络。(b)的话是把对比的样本全部存到一个memory bank中，训练的时候之间从memory bank中采样。</p><p>（c）就是MoCo的做法，与（a）不同的是，右边的 Encoder 是不直接通过反向传播来训练的，而是优化器产生的动量更新，更新的表达式如下。</p><p>$$<br>\theta_{\mathrm{k}} \leftarrow m \theta_{\mathrm{k}}+(1-m) \theta_{\mathrm{q}}<br>\tag{4}<br>$$</p><p>θ_k 是右边 Encoder 的参数，m默认设为0.999，θ_q 是左边编码 query 的 Encoder，θ_q 通过反向传播来更新，θ_k 则是通过 θ_q 动量更新。为什么采用这样的方式来更新？论文给出的解释是 θ_k 直接通过反向传播来更新的效果并不好，因为 θ_k 快速的变化会导致 key 的表征不稳定，但是动量更新很好地解决了这个问题。</p><p>现在的 Momentum Encoder 的更新是通过4式，以动量的方法更新的，不涉及反向传播，所以 输入的负样本 (negative samples) 的数量可以很多，具体就是 Queue 的大小可以比较大，那当然是负样本的数量越多越好了。这就是 Dictionary as a queue 的含义，即通过动量更新的形式，使得可以包含更多的负样本。而且 Momentum Encoder 的更新极其缓慢，所以Momentum Encoder 的更新相当于是看了很多的 Batch，也就是很多负样本。</p><p>MoCo的每个step都会更新Momentum Encoder，虽然更新缓慢，但是每个step都会通过式（4）更新 Momentum Encoder，这样 Encoder 和 Momentum Encoder 每个step 都有更新，就解决了一致性的问题。</p><h1 id="MoCo-V1算法理解"><a href="#MoCo-V1算法理解" class="headerlink" title="MoCo V1算法理解"></a>MoCo V1算法理解</h1><p>如果还没有了解清楚的话，可以来看下算法训练的伪代码，也许会更清晰一点。</p><p><img src="/images/mocov1/moco3.png"></p><ol><li>数据增强：</li></ol><p>现在我们有一堆无标签的数据，拿出一个 Batch，代码表示为 x，也就是 张图片，分别进行两种不同的数据增强，得到 x_q 和 x_k，则 x_q 是 张图片，x_k 也是 张图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> loader: <span class="hljs-comment"># 输入一个图像序列x，包含N张图，没有标签</span><br>    x_q = aug(x) <span class="hljs-comment"># 查询queue的图 (数据增强得到)    </span><br>    x_k = aug(x) <span class="hljs-comment"># 模板图 (数据增强得到)</span><br></code></pre></td></tr></table></figure><ol start="2"><li>分别通过 Encoder 和 Momentum Encoder：</li></ol><p>x_q 通过 Encoder 得到特征 q，维度是 NxC，这里特征空间由一个长度为 C&#x3D;128 的向量表示。</p><p>x_k 通过 Momentum Encoder 得到特征 k，维度是 NxC。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">q = f_q.forward(x_q) <span class="hljs-comment"># 提取查询特征，输出NxC    </span><br>k = f_k.forward(x_k) <span class="hljs-comment"># 提取模板特征，输出NxC</span><br></code></pre></td></tr></table></figure><ol start="3"><li>Momentum Encoder的参数不更新：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 不使用梯度更新f_k的参数，假设用于提取模板的表示应该是稳定的，不应立即更新    </span><br>k = k.detach()<br></code></pre></td></tr></table></figure><ol start="4"><li>计算 N 张图片的自己与自己的增强图的特征的匹配度：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 这里bmm是分批矩阵乘法，输出Nx1，也就是自己与自己的增强图的特征的匹配度</span><br>l_pos = bmm(q.view(N,<span class="hljs-number">1</span>,C), k.view(N,C,<span class="hljs-number">1</span>)) <br></code></pre></td></tr></table></figure><p>这里得到的 l_pos 的维度是 (N, 1, 1)，N 代表 N 张图片的自己与自己的增强图的特征的匹配度。</p><ol start="5"><li>计算 N 张图片与队列中的 K 张图的特征的匹配度：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 输出Nxk，自己与上一批次所有图的匹配度（全不匹配）</span><br>l_neg = mm(q.view(N,C), queue.view(C,K)) <br></code></pre></td></tr></table></figure><p>这里得到的 l_neg 的维度是 (N, K)，代表 N 张图片与队列 Queue 中的 K 张图的特征的匹配度。</p><ol start="6"><li>把 4, 5 两步得到的结果concat起来：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">logits = cat([l_pos, l_neg], dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># 输出 Nx(1+k)</span><br></code></pre></td></tr></table></figure><p>这里得到的 logits 的维度是 (N, K+1)，把它看成是一个矩阵的话呢，有 N 行，代表一个 Batch Size 里面的 N 张图片。每一行的第1个元素是某张图片自己与自己的匹配度。</p><ol start="7"><li>NCE损失函数，就是为了保证自己与自己衍生的匹配度输出越大越好，否则越小越好：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python">labels = zeros(N)<br><br><span class="hljs-comment"># NCE损失函数，就是为了保证自己与自己衍生的匹配度输出越大越好，否则越小越好</span><br>loss = CrossEntropyLoss(logits/t, labels)<br>loss.backward()<br></code></pre></td></tr></table></figure><ol start="8"><li>更新 Encoder 的参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">update(f_q.params) <span class="hljs-comment"># f_q 使用梯度立即更新</span><br></code></pre></td></tr></table></figure><ol start="9"><li>Momentum Encoder 的参数使用动量更新：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 这里使用动量法更新</span><br>f_k.params = m * f_k.params + (<span class="hljs-number">1</span> - m) * f_q.params<br></code></pre></td></tr></table></figure><ol start="10"><li>更新队列，删除最老的一个 Batch，加入一个新的 Batch：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">enqueue(queue, k) <span class="hljs-comment"># 为了生成反例，所以引入了队列</span><br>dequeue(queue)<br></code></pre></td></tr></table></figure><h1 id="MoCo-V1-对比实验"><a href="#MoCo-V1-对比实验" class="headerlink" title="MoCo V1 对比实验"></a>MoCo V1 对比实验</h1><ol><li><strong>实验一：Linear Classification Protocol</strong></li></ol><p>评价一个自监督模型的性能，最关键和最重要的实验莫过于 Linear Classification Protocol 了，它也叫做 Linear Evaluation，具体做法就是先使用自监督的方法预训练 Encoder，这一过程不使用任何 label。预训练完以后 Encoder 部分的权重也就确定了，这时候把它的权重冻结住，同时在 Encoder 的末尾添加 Global Average Pooling 和一个线性分类器 (FC+softmax)，并在固定数据集上做 Fine-tune，这一过程使用全部的 label。</p><p>上述方法在（a）原始的端到端自监督学习方法，（b）采用一个较大的memory bank存储较大的字典，（c）MoCo方法的结果对比如下图。</p><p><img src="/images/mocov1/experience2.png"></p><p>看到图中的3条曲线都是随着 K 的增加而上升的，证明对于每一个样本来讲，正样本的数量都是一个，随着负样本数量的上升，自监督训练的性能会相应提升。我们看图中的黑色线（a）最大取到了1024，因为这种方法同时使用反向传播更新 Encoder 和 Encoder 的参数，所以 Batch size 的大小受到了显存容量的限制。同时橙色曲线是最优的，证明了MoCo方法的有效性。</p><ol start="2"><li><strong>实验四：下游任务 Fine-tune 结果</strong></li></ol><p>有了预训练好的模型，就相当于是已经把参数训练到了初步成型，这时候再根据下游任务 (Downstream Tasks) 的不同去用带标签的数据集把参数训练到完全成型，那这时用的数据集量就不用太多了，因为参数经过了第1阶段就已经训练得差不多了。</p><p>本文的下游任务是：PASCAL VOC Object Detection 以及 COCO Object Detection and Segmentation，主要对比的对象是 ImageNet 预训练模型 (ImageNet supervised pre-training)，注意这个模型是使用100%的 ImageNet 标签训练的。</p><p>如下图是在 trainval07+12 (约16.5k images) 数据集上 Fine-tune 之后的结果，当Backbone 使用 R50-dilated-C5 时，在 ImageNet-1M 上预训练的 MoCo 模型的性能与有监督学习的性能是相似的。在 Instagram-1B 上预训练的 MoCo 模型的性能超过了有监督学习的性能。当Backbone 使用 R50-dilated-C5 时，在 ImageNet-1M 或者 Instagram-1B 上预训练的 MoCo 模型的性能都超过了有监督学习的性能。</p><p><img src="/images/mocov1/experience4.png"></p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] Hadsell, Raia, Sumit Chopra, and Yann LeCun. “Dimensionality reduction by learning an invariant mapping.” 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). Vol. 2. IEEE, 2006.</p><p>[2] Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” International conference on machine learning. PMLR, 2020.</p><p>[3] He, Kaiming, et al. “Momentum contrast for unsupervised visual representation learning.” Proceedings of the IEEE&#x2F;CVF conference on computer vision and pattern recognition. 2020.</p><p>[4] <a href="https://zhuanlan.zhihu.com/p/364446773">https://zhuanlan.zhihu.com/p/364446773</a></p><p>[5] <a href="https://zhuanlan.zhihu.com/p/469100381">https://zhuanlan.zhihu.com/p/469100381</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeiT：注意力Attention也能蒸馏</title>
    <link href="/2022/07/20/deit/"/>
    <url>/2022/07/20/deit/</url>
    
    <content type="html"><![CDATA[<p>《Training data-efﬁcient image transformers &amp; distillation through attention》</p><p>ViT 在大数据集 ImageNet-21k（14million）或者 JFT-300M（300million） 上进行训练，Batch Size 128 下 NVIDIA A100 32G GPU 的计算资源加持下预训练 ViT-Base&#x2F;32 需要3天时间。</p><p>Facebook 与索邦大学 Matthieu Cord 教授合作发表 Training data-efficient image transformers（DeiT） &amp; distillation through attention，DeiT 模型（8600万参数）仅用一台 GPU 服务器在 53 hours train，20 hours finetune，仅使用 ImageNet 就达到了 84.2 top-1 准确性，而无需使用任何外部数据进行训练。性能与最先进的卷积神经网络（CNN）可以抗衡。所以呢，很有必要讲讲这个 DeiT 网络模型的相关内容。</p><p>下面来简单总结 DeiT：</p><blockquote><p>DeiT 是一个全 Transformer 的架构。其核心是提出了针对 ViT 的教师-学生蒸馏训练策略，并提出了 token-based distillation 方法，使得 Transformer 在视觉领域训练得又快又好。</p></blockquote><p><img src="/images/deit/experience1.png"></p><h1 id="DeiT-相关背景"><a href="#DeiT-相关背景" class="headerlink" title="DeiT 相关背景"></a>DeiT 相关背景</h1><p>ViT 文中表示数据量不足会导致 ViT 效果变差。针对以上问题，DeiT 核心共享是使用了蒸馏策略，能够仅使用 ImageNet-1K 数据集就就可以达到 83.1% 的 Top1。</p><p>那么文章主要贡献可以总结为三点：</p><ol><li>仅使用 Transformer，不引入 Conv 的情况下也能达到 SOTA 效果。</li><li>提出了基于 token 蒸馏的策略，针对 Transformer 蒸馏方法超越传统蒸馏方法。</li><li>DeiT 发现使用 Convnet 作为教师网络能够比使用 Transformer 架构效果更好。</li></ol><p>正式了解 DeiT 算法之前呢，有几个问题需要去了解的：ViT的缺点和局限性，为什么训练ViT要准备这么多数据，就不能简单快速训练一个模型出来吗？另外 Transformer 视觉模型又怎么玩蒸馏呢？</p><h2 id="ViT-的缺点和局限性"><a href="#ViT-的缺点和局限性" class="headerlink" title="ViT 的缺点和局限性"></a>ViT 的缺点和局限性</h2><p>Transformer的输入是一个序列（Sequence），ViT 所采用的思路是把图像分块（patches），然后把每一块视为一个向量（vector），所有的向量并在一起就成为了一个序列（Sequence），ViT 使用的数据集包括了一个巨大的包含了 300 million images的 JFT-300，这个数据集是私有的，即外部研究者无法复现实验。而且在ViT的实验中作者明确地提到：</p><blockquote><p>“That transformers do not generalize well when trained on insufficient amounts of data.”</p></blockquote><p><img src="/images/deit/transformer1.png" alt="ViT"></p><p>意思是当不使用 JFT-300 大数据集时，效果不如CNN模型。也就反映出Transformer结构若想取得理想的性能和泛化能力就需要这样大的数据集。DeiT 作者通过所提出的蒸馏的训练方案，只在 Imagenet 上进行训练，就产生了一个有竞争力的无卷积 Transformer。</p><h3 id="Visual-transformer"><a href="#Visual-transformer" class="headerlink" title="Visual transformer"></a>Visual transformer</h3><p><strong>Multi-head Self Attention layers (MSA)：</strong></p><p>首先有一个 Query 矩阵 Q 和一个 Key 矩阵 K，把二者矩阵乘在一起并进行归一化以后得到 attention 矩阵，它再与Value矩阵 V 相乘得到最终的输出得到 Z。最后经过 linear transformation 得到 NxD 的输出结果。</p><p><img src="/images/deit/transformer2.png" alt="Multi-head Self Attention layers (MSA)"></p><p><strong>Feed-Forward Network (FFN)：</strong></p><p>Multi-head Self Attention layers 之后往往会跟上一个 Feed-Forward Network (FFN) ，它一般是由2个linear layer构成，第1个linear layer把维度从 D 维变换到 ND 维，第2个linear layer把维度从 ND 维再变换到 D 维。</p><p>此时 Transformer block 是不考虑位置信息的，基于此 ViT 加入了位置编码 (Positional Encoding)，这些编码在第一个 block 之前被添加到 input token 中代表位置信息，作为额外可学习的embedding（Exgra learnable class embedding）。</p><p><strong>Class token：</strong></p><p>Class token 与 input token 并在一起输入 Transformer block 中，最后的输出结果用来预测类别。这样一来，Transformer 相当于一共处理了 N+1 个维度为 D 的token，并且只有第一个 token 的输出用来预测类别。</p><h2 id="知识蒸馏介绍"><a href="#知识蒸馏介绍" class="headerlink" title="知识蒸馏介绍"></a>知识蒸馏介绍</h2><p>Knowledge Distillation（KD）最初被 Hinton 提出 “Distilling the Knowledge in a Neural Network”，与 Label smoothing 动机类似，但是 KD 生成 soft label 的方式是通过教师网络得到的。</p><p>KD 可以视为将教师网络学到的信息压缩到学生网络中。还有一些工作 “Circumventing outlier of autoaugment with knowledge distillation” 则将 KD 视为数据增强方法的一种。</p><h3 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h3><p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性。在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下:</p><ul><li>推理速度和性能慢</li><li>对部署资源要求高(内存，显存等)</li></ul><p>在部署时，对延迟以及计算资源都有着严格的限制。因此，模型压缩（在保证性能的前提下减少模型的参数量）成为了一个重要的问题，而“模型蒸馏”属于模型压缩的一种方法。</p><h3 id="理论原理"><a href="#理论原理" class="headerlink" title="理论原理"></a>理论原理</h3><p>知识蒸馏使用的是 Teacher—Student 模型，其中 Teacher 是“知识”的输出者，Student 是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p><ol><li><strong>原始模型训练</strong>: 训练 “Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li><li><strong>精简模型训练</strong>: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li></ol><p>论文中，Hinton 将问题限定在分类问题下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。知识蒸馏时，由于已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。</p><p>其中KD的训练过程和传统的训练过程的对比：</p><ol><li>传统training过程 <strong>Hard Targets</strong>: 对 ground truth 求极大似然 Softmax 值。</li><li>KD的training过程 <strong>Soft Targets</strong>: 用 Teacher 模型的 class probabilities作为soft targets。</li></ol><p><img src="/images/deit/kd1.png"></p><p>这就解释了为什么通过蒸馏的方法训练出的 Net-S 相比使用完全相同的模型结构和训练数据只使用Hard Targets的训练方法得到的模型，拥有更好的泛化能力。</p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p>第一步是训练Net-T；第二步是在高温 T 下，蒸馏 Net-T 的知识到 Net-S。</p><p><img src="/images/deit/kd2.png"></p><p>训练 Net-T 的过程很简单，而高温蒸馏过程的目标函数由distill loss（对应soft target）和student loss（对应hard target）加权得到：</p><p>$$<br>L&#x3D;\alpha L_{soft}+\beta L_{hard}<br>$$</p><p>Deit 中使用 Conv-Based 架构作为教师网络，以 soft 的方式将归纳偏置传递给学生模型，将局部性的假设通过蒸馏方式引入 Transformer 中，取得了不错的效果。</p><h1 id="DeiT-具体方法"><a href="#DeiT-具体方法" class="headerlink" title="DeiT 具体方法"></a>DeiT 具体方法</h1><p>为什么DeiT能在大幅减少 <strong>1. 训练所需的数据集</strong> 和 <strong>2. 训练时长</strong> 的情况下依旧能够取得很不错的性能呢？我们可以把这个原因归结为DeiT的训练策略。ViT 在小数据集上的性能不如使用CNN网络 EfficientNet，但是跟ViT结构相同，仅仅是使用更好的训练策略的DeiT比ViT的性能已经有了很大的提升，在此基础上，再加上蒸馏 (distillation) 操作，性能超过了 EfficientNet。</p><p>假设有一个性能很好的分类器作为teacher model，通过引入了一个 Distillation Token，然后在 self-attention layers 中跟 class token，patch token 在 Transformer 结构中不断学习。</p><p>Class token的目标是跟真实的label一致，而Distillation Token是要跟teacher model预测的label一致。</p><p><img src="/images/deit/deit1.png" alt="DeiT结构"></p><p>对比 ViT 的输出是一个 softmax，它代表着预测结果属于各个类别的概率的分布。ViT的做法是直接将 softmax 与 GT label取 CE Loss。</p><p>$$<br>CELoss(x, y) &#x3D; - \sum y_i * log(x_i)<br>$$</p><p>而在 DeiT 中，除了 CE Loss 以外，还要 1）定义蒸馏损失；2）加上 Distillation Token。</p><ol><li><strong>定义蒸馏损失</strong></li></ol><p>蒸馏分两种，一种是软蒸馏（soft distillation），另一种是硬蒸馏（hard distillation）。软蒸馏如下式所示，Z_s 和 Z_t 分别是 student model 和 teacher model 的输出，KL 表示 KL 散度，psi 表示softmax函数，lambda 和 tau 是超参数：</p><p>$$<br>\mathcal{L}<em>{\text {global }}&#x3D;(1-\lambda) \mathcal{L}</em>{\mathrm{CE}}\left(\psi\left(Z_{\mathrm{s}}\right), y\right)+\lambda \tau^{2} \mathrm{KL}\left(\psi\left(Z_{\mathrm{s}} &#x2F; \tau\right), \psi\left(Z_{\mathrm{t}} &#x2F; \tau\right)\right)<br>$$</p><p>硬蒸馏如下式所示，其中 CE 表示交叉熵：</p><p>$$<br>\mathcal{L}<em>{\text {global }}^{\text {hardDistill }}&#x3D;\frac{1}{2} \mathcal{L}</em>{\mathrm{CE}}\left(\psi\left(Z_{s}\right), y\right)+\frac{1}{2} \mathcal{L}<em>{\mathrm{CE}}\left(\psi\left(Z</em>{s}\right), y_{\mathrm{t}}\right)<br>$$</p><p>学生网络的输出 Z_s 与真实标签之间计算 CE Loss 。如果是硬蒸馏，就再与教师网络的标签取 CE Loss。如果是软蒸馏，就再与教师网络的 softmax 输出结果取 KL Loss 。</p><p>值得注意的是，Hard Label 也可以通过标签平滑技术 （Label smoothing） 转换成Soft Labe，其中真值对应的标签被认为具有 1- esilon 的概率，剩余的 esilon 由剩余的类别共享。</p><ol start="2"><li><strong>加入 Distillation Token</strong></li></ol><p>Distillation Token 和 ViT 中的 class token 一起加入 Transformer 中，和class token 一样通过 self-attention 与其它的 embedding 一起计算，并且在最后一层之后由网络输出。</p><p>而 Distillation Token 对应的这个输出的目标函数就是蒸馏损失。Distillation Token 允许模型从教师网络的输出中学习，就像在常规的蒸馏中一样，同时也作为一种对class token的补充。</p><p><img src="/images/deit/deit2.png" alt="DeiT训练流程"></p><h1 id="DeiT-具体实验"><a href="#DeiT-具体实验" class="headerlink" title="DeiT 具体实验"></a>DeiT 具体实验</h1><p>实验参数的设置：图中表示不同大小的 DeiT 结构的超参数设置，最大的结构是 DeiT-B，与 ViT-B 结构是相同，唯一不同的是 embedding 的 hidden dimension 和 head 数量。作者保持了每个head的隐变量维度为64，throughput是一个衡量DeiT模型处理图片速度的变量，代表每秒能够处理图片的数目。</p><p><img src="/images/deit/experience2.png" alt="不同大小的DeiT结构的超参数设置"></p><ol><li><strong>Teacher model对比</strong></li></ol><p>作者首先观察到使用 CNN 作为 teacher 比 transformer 作为 teacher 的性能更优。下图中对比了 teacher 网络使用 DeiT-B 和几个 CNN 模型 RegNetY 时，得到的 student 网络的预训练性能以及 finetune 之后的性能。</p><p>其中，DeiT-B 384 代表使用分辨率为 384×384 的图像 finetune 得到的模型，最后的那个小蒸馏符号 alembic sign 代表蒸馏以后得到的模型。</p><p><img src="/images/deit/experience3.png" alt="不同teacher模型的性能指标对比"></p><ol start="2"><li><strong>蒸馏方法对比</strong></li></ol><p>下图是不同蒸馏策略的性能对比，label 代表有监督学习，前3行分别是不使用蒸馏，使用soft蒸馏和使用hard蒸馏的性能对比。前3行不使用 Distillation Token 进行训练，只是相当于在原来 ViT 的基础上给损失函数加上了蒸馏部分。</p><p>对于Transformer来讲，硬蒸馏的性能明显优于软蒸馏，即使只使用 class token，不使用 distill token，硬蒸馏达到 83.0%，而软蒸馏的精度为 81.8%。  </p><p><img src="/images/deit/experience4.png" alt="不同蒸馏策略的性能对比"></p><p>从最后两列 B224 和 B384 看出，以更高的分辨率进行微调有助于减少方法之间的差异。这可能是因为在微调时，作者不使用教师信息。随着微调，class token 和 Distillation Token 之间的相关性略有增加。</p><p>除此之外，蒸馏模型在 accuracy 和 throughput 之间的 trade-off 甚至优于 teacher 模型，这也反映了蒸馏的有趣之处。</p><ol start="3"><li><strong>性能对比</strong></li></ol><p>下面是不同模型性能的数值比较。可以发现在参数量相当的情况下，卷积网络的速度更慢，这是因为大的矩阵乘法比小卷积提供了更多的优化机会。EffcientNet-B4和DeiT-B alembic sign的速度相似，在3个数据集的性能也比较接近。</p><p><img src="/images/deit/experience5.png" alt="不同模型性能的数值比较"></p><ol start="4"><li><strong>对比实验</strong></li></ol><p>作者还做了一些关于数据增强方法和优化器的对比实验。Transformer的训练需要大量的数据，想要在不太大的数据集上取得好性能，就需要大量的数据增强，以实现data-efficient training。几乎所有评测过的数据增强的方法都能提升性能。对于优化器来说，AdamW比SGD性能更好。</p><p>此外，发现Transformer对优化器的超参数很敏感，试了多组 lr 和 weight+decay。stochastic depth有利于收敛。Mixup 和 CutMix 都能提高性能。Exp.+Moving+Avg. 表示参数平滑后的模型，对性能提升只是略有帮助。最后就是 Repeated augmentation 的数据增强方式对于性能提升帮助很大。</p><p><img src="/images/deit/experience6.png" alt="对比实验"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>DeiT 模型（8600万参数）仅用一台 GPU 服务器在 53 hours train，20 hours finetune，仅使用 ImageNet 就达到了 84.2 top-1 准确性，而无需使用任何外部数据进行训练，性能与最先进的卷积神经网络（CNN）可以抗衡。其核心是提出了针对 ViT 的教师-学生蒸馏训练策略，并提出了 token-based distillation 方法，使得 Transformer 在视觉领域训练得又快又好。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] <a href="https://zhuanlan.zhihu.com/p/349315675">https://zhuanlan.zhihu.com/p/349315675</a></p><p>[2] <a href="http://giantpandacv.com/academic/%E7%AE%97%E6%B3%95%E7%A7%91%E6%99%AE/Transformer/DeiT%EF%BC%9A%E4%BD%BF%E7%94%A8Attention%E8%92%B8%E9%A6%8FTransformer/">DeiT：使用Attention蒸馏Transformer</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/102038521">https://zhuanlan.zhihu.com/p/102038521</a></p><p>[4] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 2.7 (2015).</p><p>[5] Touvron, Hugo, et al. “Training data-efficient image transformers &amp; distillation through attention.” International Conference on Machine Learning. PMLR, 2021.</p><p>[6] Dosovitskiy, Alexey, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</p><p>[7] Wei, Longhui, et al. “Circumventing outliers of autoaugment with knowledge distillation.” European Conference on Computer Vision. Springer, Cham, 2020.</p>]]></content>
    
    
    
    <tags>
      
      <tag>DeiT</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
